{
  "hash": "2099aef9787b2dc79dc944fa9881748d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: pump impeller trimming draft-01\nauthor:\n  - name: Mohamed Tawheed\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: moh@example\n    roles:\n      - Investigation\n      - Project administration\n      - Software\n      - Visualization\n    affiliations:\n      - Curvenote\n  - name: Seif Ibrahim\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n      - alexandria university school of mechanicaL engineering\nkeywords:\n  - Pump impeller trimming\n  - Neural networks\n  - Hyperparameter optimization\n  - Genetic algorithms\n  - Mean squared error\nabstract: |\n  This paper presents a genetic algorithm (GA) methodology to optimize neural network hyperparameters in the context of pump impeller trimming. Impeller trimming, a process involving modifications to pump impeller geometry, traditionally requires expert knowledge and empirical methods to achieve the desired performance. The use of neural networks (NNs) provides an automated approach to improve the impeller trimming process based on input data and performance outcomes. The proposed method uses a GA to identify the optimal NN hyperparameters, such as hidden layer size, training function, activation function, and maximum epochs, aiming to minimize the mean squared error (MSE) between the network's predictions and the actual target outcomes. This paper discusses the implementation details of the optimization process and explains the key components and their significance.\nplain-language-summary: |\n  pump trimming ...\nkey-points: null\ndate: last-modified\nbibliography: references.bib\nnocite: '@*'\ncitation:\n  container-title: bachelor project with prof. dr. mohamed farid khalil\nnumber-sections: true\n---\n\n:::{#76cee825 .cell .markdown}\n# introduction\n\nPump impeller trimming is a critical procedure in optimizing pump performance for specific applications. It involves modifying the impeller's geometry to achieve desired hydraulic characteristics such as head, flow rate, and efficiency. Traditionally, the process has been dependent on empirical methods and engineering expertise. However, the introduction of artificial neural networks (NNs) offers a data-driven approach to automate and enhance impeller trimming.\n\nNNs excel at modeling complex relationships between input data and desired outputs. In the case of pump impeller trimming, the input data (x) can include geometric parameters of the impeller, while the target data (t) can consist of pump performance metrics. By training an NN on a dataset of impeller designs and performance outcomes, the network can learn to predict new impeller performance based on their geometries.\n\nAchieving optimal NN performance requires selecting appropriate hyperparameters, which influence network architecture and the learning process. Key NN hyperparameters include the size of the hidden layer, the training function for weight updates, the activation function introducing non-linearity, and the maximum number of training epochs.\n\n\n# Methodology\n\nThis paper outlines a genetic algorithm (GA) methodology for optimizing neural network hyperparameters in pump impeller trimming. GAs are suitable for searching for optimal solutions in complex, high-dimensional spaces. The GA approach used in this study involves a population of candidate hyperparameter sets. Each set is evaluated by training an NN with those hyperparameters and measuring the resulting MSE on a validation dataset. The GA iteratively selects promising hyperparameter sets based on MSE values, performs crossover and mutation to create new candidates, and continues until a stopping criterion (such as maximum generations or elapsed time) is met.\n\n\n## Define Options\n\n Define the set of available training functions (e.g., Levenberg-Marquardt, Bayesian Regularization) and activation functions (e.g., tansigmoid, logarithmic sigmoid).\n\n\n## Nested Evaluation Function\n\nImplement a nested function `evaluateHyperparameters(hyperParams, x, t, randomSeed)` to evaluate a candidate hyperparameter set (`hyperParams`), input data (`x`), and target data (`t`). This function:\n- Extracts individual hyperparameters (hidden layer size, maximum epochs, training and activation function indices).\n- Defines an NN architecture with the given hyperparameters.\n- Splits data into training, validation, and testing sets.\n- Trains the NN using the training data and specified training function.\n- Evaluates the NN on validation data and calculates MSE.\n- Returns the calculated MSE.\n\n\n\n## Random Seed and Bounds\n\nset a random seed for reproducibility and define bounds for each hyperparameter based on practical considerations and prior knowledge.\n\n### Importance of Random Seed:\n\n- **Reproducibility:** Setting a random seed ensures that the neural network training process is reproducible. This allows for consistent results across different runs and helps in comparing models fairly.\n- **Comparison:** When testing different hyperparameters or models, using the same random seed allows for a direct comparison of their performance.\n\n## GA Options\n\n Configure GA options using `optimoptions('ga')`, specifying parameters such as maximum allowed time.\n\n##  Perform Optimization\n\n Execute the GA using `ga` from MATLAB's Optimization Toolbox. This function uses the `evaluateHyperparameters` function as the objective function to minimize, along with the number of hyperparameters, bounds, and GA options.\n\n##  Extract and Round\n Obtain the final optimized hyperparameter set and round them (if necessary) for practical NN implementation.\n\n##  Report Results\n\n Report the optimized hyperparameters, final MSE, random seed used, and total elapsed time of the optimization process. \n\n\n\n\n# Conclusion\n\n This paper presents a GA-based approach for optimizing neural network hyperparameters in the context of pump impeller trimming. The methodology enables a data-driven, automated optimization process, providing potential improvements in efficiency and performance in pump design and operation. Future work may explore refining the genetic algorithm for better convergence or testing the approach with more complex NN architectures.\n\n\n\n\n\n\n# code documentation\n\n\n\n**Optimizing Neural Network Hyperparameters for Pump Impeller Trimming**\n\nThis code implements a function called `optimizeNNForTrimmingPumpImpeller` that uses a Genetic Algorithm (GA) to optimize the hyperparameters of a neural network for pump impeller trimming. \n\n\n**1. Starting Timer and User Notification:**\n\n```matlab\n    % Start timer to measure the duration of the optimization process.\n    tic;\n    disp(\"Optimization in progress. This process may take up to 30 seconds...\");\n```\n\n- The `tic` function starts a timer to measure how long the optimization process takes.\n- The `disp` function displays a message to the user indicating that the optimization is underway and might take up to 30 seconds.\n\n**2. Defining Training and Activation Function Options:**\n\n```matlab\n% Define the available options for training functions and activation functions.\n    trainingFunctionOptions = {'trainlm', 'trainbr', ...\n    'trainrp', 'traincgb', 'traincgf',...\n     'traincgp', 'traingdx', 'trainoss'};\n    activationFunctionOptions = {'tansig', 'logsig'};\n```\n\n- This section defines two cell arrays.\n  - `trainingFunctionOptions`: This array stores names of various training functions available in MATLAB's `fitnet` object. Training functions determine how the neural network updates its weights during training, impacting its learning behavior.\n  - `activationFunctionOptions`: This array stores names of activation functions that can be applied in the neural network's hidden layer. Activation functions introduce non-linearity, allowing the network to model complex relationships between inputs and outputs.\n\n**3. Defining Bounds and Options for Genetic Algorithm (GA):**\n\n```matlab\n    % Define bounds for the genetic algorithm optimization.\n    lowerBounds = [5, 50, 1, 1];\n    upperBounds = [200, 200, 8, 2];\n\n    % Define options for the genetic algorithm.\n    gaOptions = optimoptions('ga', 'MaxTime', 20);\n```\n\n- **Bounds for GA Search (`lowerBounds` and `upperBounds`)**\n   - These vectors define the minimum and maximum allowable values for each hyperparameter during the GA optimization. For instance, `lowerBounds = [5, 50, 1, 1]` specifies a minimum hidden layer size of 5 neurons, a minimum of 50 training epochs, and minimum indices of 1 for the training function and activation function (since these indices correspond to entries within the options arrays we defined earlier). The upper bounds define the maximum allowed values for each hyperparameter.\n\n- **GA Options (`gaOptions`)**\n   - This line uses `optimoptions('ga')` to create a structure containing options for the GA. The GA is a search algorithm inspired by biological evolution. It iteratively tweaks candidate solutions (in our case, sets of hyperparameters) and selects promising ones based on their performance (MSE in this case) to create new generations. You can modify these options to control aspects like population size (number of candidate hyperparameter sets considered simultaneously) and termination criteria (when to stop the search). Here, `'MaxTime', 20` sets a maximum allowed time of 20 seconds for the optimization process per iteration (not total time).\n   The algorithm stops after running for MaxTime seconds, as measured by tic and toc. This limit is enforced after each iteration, so ga can exceed the limit when an iteration takes substantial time.\n\n**4. Global Variable to Store Best Network:**\n\n```matlab\n    % Global variable to store the best trained neural network found during optimization.\n    global bestTrainedNet;\n    bestTrainedNet = [];\n```\n\n- A global variable named `bestTrainedNet` is declared. This variable will be used to store the neural network model that achieves the lowest MSE during the optimization process.\n\n**5. Nested Function: evaluateHyperparameters**\n\n```matlab\n% Nested function to evaluate hyperparameters using the neural network.\n    function mse = evaluateHyperparameters(hyperParams, x, t, randomSeed)\n        rng(randomSeed); % Set random seed for reproducibility.\n\n        % Extract hyperparameters.\n        hiddenLayerSize = round(hyperParams(1)); %Hidden Layer Size\n        maxEpochs = round(hyperParams(2));       %Max Epochs\n        trainingFunctionIdx = round(hyperParams(3)); %Training Function\n        activationFunctionIdx = round(hyperParams(4));%Activation \n        %Function or transfere function\n\n        % Define the neural network.\n        net = fitnet(hiddenLayerSize, trainingFunctionOptions{trainingFunctionIdx});\n        net.trainParam.showWindow = false; % Suppress training GUI for efficiency.\n        net.trainParam.epochs = maxEpochs;\n        net.layers{1}.transferFcn = activationFunctionOptions{activationFunctionIdx};\n\n        % Define data split for training, validation, and testing.\n        net.divideParam.trainRatio = 0.7;\n        net.divideParam.valRatio = 0.15;\n        net.divideParam.testRatio = 0.15;\n\n        % Train the neural network.\n        [trainedNet, ~] = train(net, x, t);\n\n        % Evaluate the model performance using mean squared error (MSE).\n        predictions = trainedNet(x);\n        mse = perform(trainedNet, t, predictions);\n\n        % Check if the current MSE is the best MSE so far \n        %and update the global variable if necessary.\n        if isempty(bestTrainedNet) || mse < perform(bestTrainedNet,...\n         t, bestTrainedNet(x))\n            bestTrainedNet = trainedNet;\n        end\n    end\n\n ```   \n\n\nThe function `evaluateHyperparameters` evaluates a set of hyperparameters for a neural network model. It uses the hyperparameters to define, train, and evaluate a neural network, returning the model's mean squared error (MSE) as a measure of performance. \n\n### Function Overview:\n\n1. **Inputs:**\n   - `hyperParams`: A vector of hyperparameters including:\n     - `hiddenLayerSize`: The size of the hidden layer in the neural network.\n     - `maxEpochs`: The maximum number of epochs (training iterations).\n     - `trainingFunctionIdx`: The index of the training function to use.\n     - `activationFunctionIdx`: The index of the activation function to use.\n   - `x`: The input data (features) for the neural network.\n   - `t`: The target data (labels) for the neural network.\n   - `randomSeed`: The random seed for reproducibility.\n\n2. **Set Random Seed:**\n   - The function starts by setting the random seed using `rng(randomSeed)` for reproducibility. This ensures that the neural network training process is consistent and repeatable.\n\n3. **Extract and Apply Hyperparameters:**\n   - The function extracts hyperparameters from the input vector `hyperParams`.\n   - It uses these hyperparameters to define the neural network architecture, training function, and activation function.\n\n4. **Data Splitting:**\n   - The function defines how to split the data into training, validation, and testing sets (70% for training, 15% for validation, and 15% for testing).\n\n5. **Train the Neural Network:**\n   - The function trains the neural network using the specified hyperparameters and input data.\n\n6. **Evaluate Performance:**\n   - The function evaluates the trained network's performance using mean squared error (MSE), a measure of prediction accuracy.\n\n7. **Track the Best Trained Network:**\n   - The function compares the current MSE to the best MSE seen so far. If the current MSE is better, the trained network is stored as the best-trained network.\n\n### Importance of Random Seed:\n\n- **Reproducibility:** Setting a random seed ensures that the neural network training process is reproducible. This allows for consistent results across different runs and helps in comparing models fairly.\n- **Comparison:** When testing different hyperparameters or models, using the same random seed allows for a direct comparison of their performance.\n\n### Concepts Underlying Epochs:\n\n- **Epochs:** An epoch is a complete pass through the entire training dataset. During each epoch, the neural network updates its weights based on the training data.\n- **Why Search for Optimal Epochs:** The number of epochs affects how much the network learns from the data:\n  - **Too Few Epochs:** The network may not learn enough and can underfit, performing poorly on new data.\n  - **Too Many Epochs:** The network may learn too much and can overfit, performing well on the training data but poorly on new data.\n- **Optimum Number of Epochs:** An optimal number of epochs strikes a balance between underfitting and overfitting, ensuring the network generalizes well to new data.\n\n\n# test citation\n\nas dr farid said  @khalil-ApplicationArtificialNeural-2013 this is no enough\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n:::\n\n",
    "supporting": [
      "index_files\\figure-pdf"
    ],
    "filters": []
  }
}