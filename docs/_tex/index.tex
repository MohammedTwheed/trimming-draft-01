% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{agujournal2019}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={pump impeller trimming draft-01},
  pdfauthor={Mohamed Tawheed; Seif Ibrahim},
  pdfkeywords={Pump impeller trimming, Neural networks, Hyperparameter
optimization, Genetic algorithms, Mean squared error},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\journalname{bachelor project with prof. dr. mohamed farid khalil}

\draftfalse

\begin{document}
\title{pump impeller trimming draft-01}

\authors{Mohamed Tawheed\affil{1}, Seif Ibrahim\affil{2}}
\affiliation{1}{Curvenote, }\affiliation{2}{alexandria university school
of mechanicaL engineering, }
\correspondingauthor{Mohamed Tawheed}{moh@example}


\begin{abstract}
This paper presents a genetic algorithm (GA) methodology to optimize
neural network hyperparameters in the context of pump impeller trimming.
Impeller trimming, a process involving modifications to pump impeller
geometry, traditionally requires expert knowledge and empirical methods
to achieve the desired performance. The use of neural networks (NNs)
provides an automated approach to improve the impeller trimming process
based on input data and performance outcomes. The proposed method uses a
GA to identify the optimal NN hyperparameters, such as hidden layer
size, training function, activation function, and maximum epochs, aiming
to minimize the mean squared error (MSE) between the network's
predictions and the actual target outcomes. This paper discusses the
implementation details of the optimization process and explains the key
components and their significance.
\end{abstract}

\section*{Plain Language Summary}
pump trimming \ldots{}



\section{introduction}\label{introduction}

Pump impeller trimming is a critical procedure in optimizing pump
performance for specific applications. It involves modifying the
impeller's geometry to achieve desired hydraulic characteristics such as
head, flow rate, and efficiency. Traditionally, the process has been
dependent on empirical methods and engineering expertise. However, the
introduction of artificial neural networks (NNs) offers a data-driven
approach to automate and enhance impeller trimming.

NNs excel at modeling complex relationships between input data and
desired outputs. In the case of pump impeller trimming, the input data
(x) can include geometric parameters of the impeller, while the target
data (t) can consist of pump performance metrics. By training an NN on a
dataset of impeller designs and performance outcomes, the network can
learn to predict new impeller performance based on their geometries.

Achieving optimal NN performance requires selecting appropriate
hyperparameters, which influence network architecture and the learning
process. Key NN hyperparameters include the size of the hidden layer,
the training function for weight updates, the activation function
introducing non-linearity, and the maximum number of training epochs.

\section{Methodology}\label{methodology}

This paper outlines a genetic algorithm (GA) methodology for optimizing
neural network hyperparameters in pump impeller trimming. GAs are
suitable for searching for optimal solutions in complex,
high-dimensional spaces. The GA approach used in this study involves a
population of candidate hyperparameter sets. Each set is evaluated by
training an NN with those hyperparameters and measuring the resulting
MSE on a validation dataset. The GA iteratively selects promising
hyperparameter sets based on MSE values, performs crossover and mutation
to create new candidates, and continues until a stopping criterion (such
as maximum generations or elapsed time) is met.

\subsection{Define Options}\label{define-options}

Define the set of available training functions (e.g.,
Levenberg-Marquardt, Bayesian Regularization) and activation functions
(e.g., tansigmoid, logarithmic sigmoid).

\subsection{Nested Evaluation
Function}\label{nested-evaluation-function}

Implement a nested function
\texttt{evaluateHyperparameters(hyperParams,\ x,\ t,\ randomSeed)} to
evaluate a candidate hyperparameter set (\texttt{hyperParams}), input
data (\texttt{x}), and target data (\texttt{t}). This function: -
Extracts individual hyperparameters (hidden layer size, maximum epochs,
training and activation function indices). - Defines an NN architecture
with the given hyperparameters. - Splits data into training, validation,
and testing sets. - Trains the NN using the training data and specified
training function. - Evaluates the NN on validation data and calculates
MSE. - Returns the calculated MSE.

\subsection{Random Seed and Bounds}\label{random-seed-and-bounds}

set a random seed for reproducibility and define bounds for each
hyperparameter based on practical considerations and prior knowledge.

\subsubsection{Importance of Random
Seed:}\label{importance-of-random-seed}

\begin{itemize}
\tightlist
\item
  \textbf{Reproducibility:} Setting a random seed ensures that the
  neural network training process is reproducible. This allows for
  consistent results across different runs and helps in comparing models
  fairly.
\item
  \textbf{Comparison:} When testing different hyperparameters or models,
  using the same random seed allows for a direct comparison of their
  performance.
\end{itemize}

\subsection{GA Options}\label{ga-options}

Configure GA options using
\texttt{optimoptions(\textquotesingle{}ga\textquotesingle{})},
specifying parameters such as maximum allowed time.

\subsection{Perform Optimization}\label{perform-optimization}

Execute the GA using \texttt{ga} from MATLAB's Optimization Toolbox.
This function uses the \texttt{evaluateHyperparameters} function as the
objective function to minimize, along with the number of
hyperparameters, bounds, and GA options.

\subsection{Extract and Round}\label{extract-and-round}

Obtain the final optimized hyperparameter set and round them (if
necessary) for practical NN implementation.

\subsection{Report Results}\label{report-results}

Report the optimized hyperparameters, final MSE, random seed used, and
total elapsed time of the optimization process.

\section{Conclusion}\label{conclusion}

This paper presents a GA-based approach for optimizing neural network
hyperparameters in the context of pump impeller trimming. The
methodology enables a data-driven, automated optimization process,
providing potential improvements in efficiency and performance in pump
design and operation. Future work may explore refining the genetic
algorithm for better convergence or testing the approach with more
complex NN architectures.

\section{code documentation}\label{code-documentation}

\textbf{Optimizing Neural Network Hyperparameters for Pump Impeller
Trimming}

This code implements a function called
\texttt{optimizeNNForTrimmingPumpImpeller} that uses a Genetic Algorithm
(GA) to optimize the hyperparameters of a neural network for pump
impeller trimming.

\textbf{1. Starting Timer and User Notification:}

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\% Start timer to measure the duration of the optimization process.}
    \VariableTok{tic}\OperatorTok{;}
    \VariableTok{disp}\NormalTok{(}\StringTok{"Optimization in progress. This process may take up to 30 seconds..."}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The \texttt{tic} function starts a timer to measure how long the
  optimization process takes.
\item
  The \texttt{disp} function displays a message to the user indicating
  that the optimization is underway and might take up to 30 seconds.
\end{itemize}

\textbf{2. Defining Training and Activation Function Options:}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\% Define the available options for training functions and activation functions.}
    \VariableTok{trainingFunctionOptions} \OperatorTok{=}\NormalTok{ \{}\SpecialStringTok{\textquotesingle{}trainlm\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}trainbr\textquotesingle{}}\OperatorTok{,} \OperatorTok{...}
    \SpecialStringTok{\textquotesingle{}trainrp\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}traincgb\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}traincgf\textquotesingle{}}\OperatorTok{,...}
     \SpecialStringTok{\textquotesingle{}traincgp\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}traingdx\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}trainoss\textquotesingle{}}\NormalTok{\}}\OperatorTok{;}
    \VariableTok{activationFunctionOptions} \OperatorTok{=}\NormalTok{ \{}\SpecialStringTok{\textquotesingle{}tansig\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}logsig\textquotesingle{}}\NormalTok{\}}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This section defines two cell arrays.

  \begin{itemize}
  \tightlist
  \item
    \texttt{trainingFunctionOptions}: This array stores names of various
    training functions available in MATLAB's \texttt{fitnet} object.
    Training functions determine how the neural network updates its
    weights during training, impacting its learning behavior.
  \item
    \texttt{activationFunctionOptions}: This array stores names of
    activation functions that can be applied in the neural network's
    hidden layer. Activation functions introduce non-linearity, allowing
    the network to model complex relationships between inputs and
    outputs.
  \end{itemize}
\end{itemize}

\textbf{3. Defining Bounds and Options for Genetic Algorithm (GA):}

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\% Define bounds for the genetic algorithm optimization.}
    \VariableTok{lowerBounds} \OperatorTok{=}\NormalTok{ [}\FloatTok{5}\OperatorTok{,} \FloatTok{50}\OperatorTok{,} \FloatTok{1}\OperatorTok{,} \FloatTok{1}\NormalTok{]}\OperatorTok{;}
    \VariableTok{upperBounds} \OperatorTok{=}\NormalTok{ [}\FloatTok{200}\OperatorTok{,} \FloatTok{200}\OperatorTok{,} \FloatTok{8}\OperatorTok{,} \FloatTok{2}\NormalTok{]}\OperatorTok{;}

    \CommentTok{\% Define options for the genetic algorithm.}
    \VariableTok{gaOptions} \OperatorTok{=} \VariableTok{optimoptions}\NormalTok{(}\SpecialStringTok{\textquotesingle{}ga\textquotesingle{}}\OperatorTok{,} \SpecialStringTok{\textquotesingle{}MaxTime\textquotesingle{}}\OperatorTok{,} \FloatTok{20}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Bounds for GA Search (\texttt{lowerBounds} and
  \texttt{upperBounds})}

  \begin{itemize}
  \tightlist
  \item
    These vectors define the minimum and maximum allowable values for
    each hyperparameter during the GA optimization. For instance,
    \texttt{lowerBounds\ =\ {[}5,\ 50,\ 1,\ 1{]}} specifies a minimum
    hidden layer size of 5 neurons, a minimum of 50 training epochs, and
    minimum indices of 1 for the training function and activation
    function (since these indices correspond to entries within the
    options arrays we defined earlier). The upper bounds define the
    maximum allowed values for each hyperparameter.
  \end{itemize}
\item
  \textbf{GA Options (\texttt{gaOptions})}

  \begin{itemize}
  \tightlist
  \item
    This line uses
    \texttt{optimoptions(\textquotesingle{}ga\textquotesingle{})} to
    create a structure containing options for the GA. The GA is a search
    algorithm inspired by biological evolution. It iteratively tweaks
    candidate solutions (in our case, sets of hyperparameters) and
    selects promising ones based on their performance (MSE in this case)
    to create new generations. You can modify these options to control
    aspects like population size (number of candidate hyperparameter
    sets considered simultaneously) and termination criteria (when to
    stop the search). Here,
    \texttt{\textquotesingle{}MaxTime\textquotesingle{},\ 20} sets a
    maximum allowed time of 20 seconds for the optimization process per
    iteration (not total time). The algorithm stops after running for
    MaxTime seconds, as measured by tic and toc. This limit is enforced
    after each iteration, so ga can exceed the limit when an iteration
    takes substantial time.
  \end{itemize}
\end{itemize}

\textbf{4. Global Variable to Store Best Network:}

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\% Global variable to store the best trained neural network found during optimization.}
    \KeywordTok{global} \VariableTok{bestTrainedNet}\OperatorTok{;}
    \VariableTok{bestTrainedNet} \OperatorTok{=}\NormalTok{ []}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  A global variable named \texttt{bestTrainedNet} is declared. This
  variable will be used to store the neural network model that achieves
  the lowest MSE during the optimization process.
\end{itemize}

\textbf{5. Nested Function: evaluateHyperparameters}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\% Nested function to evaluate hyperparameters using the neural network.}
    \KeywordTok{function} \VariableTok{mse} \OperatorTok{=} \VariableTok{evaluateHyperparameters}\NormalTok{(}\VariableTok{hyperParams}\OperatorTok{,} \VariableTok{x}\OperatorTok{,} \VariableTok{t}\OperatorTok{,} \VariableTok{randomSeed}\NormalTok{)}
        \VariableTok{rng}\NormalTok{(}\VariableTok{randomSeed}\NormalTok{)}\OperatorTok{;} \CommentTok{\% Set random seed for reproducibility.}

        \CommentTok{\% Extract hyperparameters.}
        \VariableTok{hiddenLayerSize} \OperatorTok{=} \VariableTok{round}\NormalTok{(}\VariableTok{hyperParams}\NormalTok{(}\FloatTok{1}\NormalTok{))}\OperatorTok{;} \CommentTok{\%Hidden Layer Size}
        \VariableTok{maxEpochs} \OperatorTok{=} \VariableTok{round}\NormalTok{(}\VariableTok{hyperParams}\NormalTok{(}\FloatTok{2}\NormalTok{))}\OperatorTok{;}       \CommentTok{\%Max Epochs}
        \VariableTok{trainingFunctionIdx} \OperatorTok{=} \VariableTok{round}\NormalTok{(}\VariableTok{hyperParams}\NormalTok{(}\FloatTok{3}\NormalTok{))}\OperatorTok{;} \CommentTok{\%Training Function}
        \VariableTok{activationFunctionIdx} \OperatorTok{=} \VariableTok{round}\NormalTok{(}\VariableTok{hyperParams}\NormalTok{(}\FloatTok{4}\NormalTok{))}\OperatorTok{;}\CommentTok{\%Activation }
        \CommentTok{\%Function or transfere function}

        \CommentTok{\% Define the neural network.}
        \VariableTok{net} \OperatorTok{=} \VariableTok{fitnet}\NormalTok{(}\VariableTok{hiddenLayerSize}\OperatorTok{,} \VariableTok{trainingFunctionOptions}\NormalTok{\{}\VariableTok{trainingFunctionIdx}\NormalTok{\})}\OperatorTok{;}
        \VariableTok{net}\NormalTok{.}\VariableTok{trainParam}\NormalTok{.}\VariableTok{showWindow} \OperatorTok{=} \VariableTok{false}\OperatorTok{;} \CommentTok{\% Suppress training GUI for efficiency.}
        \VariableTok{net}\NormalTok{.}\VariableTok{trainParam}\NormalTok{.}\VariableTok{epochs} \OperatorTok{=} \VariableTok{maxEpochs}\OperatorTok{;}
        \VariableTok{net}\NormalTok{.}\VariableTok{layers}\NormalTok{\{}\FloatTok{1}\NormalTok{\}.}\VariableTok{transferFcn} \OperatorTok{=} \VariableTok{activationFunctionOptions}\NormalTok{\{}\VariableTok{activationFunctionIdx}\NormalTok{\}}\OperatorTok{;}

        \CommentTok{\% Define data split for training, validation, and testing.}
        \VariableTok{net}\NormalTok{.}\VariableTok{divideParam}\NormalTok{.}\VariableTok{trainRatio} \OperatorTok{=} \FloatTok{0.7}\OperatorTok{;}
        \VariableTok{net}\NormalTok{.}\VariableTok{divideParam}\NormalTok{.}\VariableTok{valRatio} \OperatorTok{=} \FloatTok{0.15}\OperatorTok{;}
        \VariableTok{net}\NormalTok{.}\VariableTok{divideParam}\NormalTok{.}\VariableTok{testRatio} \OperatorTok{=} \FloatTok{0.15}\OperatorTok{;}

        \CommentTok{\% Train the neural network.}
\NormalTok{        [}\VariableTok{trainedNet}\OperatorTok{,} \OperatorTok{\textasciitilde{}}\NormalTok{] }\OperatorTok{=} \VariableTok{train}\NormalTok{(}\VariableTok{net}\OperatorTok{,} \VariableTok{x}\OperatorTok{,} \VariableTok{t}\NormalTok{)}\OperatorTok{;}

        \CommentTok{\% Evaluate the model performance using mean squared error (MSE).}
        \VariableTok{predictions} \OperatorTok{=} \VariableTok{trainedNet}\NormalTok{(}\VariableTok{x}\NormalTok{)}\OperatorTok{;}
        \VariableTok{mse} \OperatorTok{=} \VariableTok{perform}\NormalTok{(}\VariableTok{trainedNet}\OperatorTok{,} \VariableTok{t}\OperatorTok{,} \VariableTok{predictions}\NormalTok{)}\OperatorTok{;}

        \CommentTok{\% Check if the current MSE is the best MSE so far }
        \CommentTok{\%and update the global variable if necessary.}
        \KeywordTok{if} \VariableTok{isempty}\NormalTok{(}\VariableTok{bestTrainedNet}\NormalTok{) }\OperatorTok{||} \VariableTok{mse} \OperatorTok{\textless{}} \VariableTok{perform}\NormalTok{(}\VariableTok{bestTrainedNet}\OperatorTok{,...}
         \VariableTok{t}\OperatorTok{,} \VariableTok{bestTrainedNet}\NormalTok{(}\VariableTok{x}\NormalTok{))}
            \VariableTok{bestTrainedNet} \OperatorTok{=} \VariableTok{trainedNet}\OperatorTok{;}
        \KeywordTok{end}
    \KeywordTok{end}
\end{Highlighting}
\end{Shaded}

The function \texttt{evaluateHyperparameters} evaluates a set of
hyperparameters for a neural network model. It uses the hyperparameters
to define, train, and evaluate a neural network, returning the model's
mean squared error (MSE) as a measure of performance.

\subsubsection{Function Overview:}\label{function-overview}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Inputs:}

  \begin{itemize}
  \tightlist
  \item
    \texttt{hyperParams}: A vector of hyperparameters including:

    \begin{itemize}
    \tightlist
    \item
      \texttt{hiddenLayerSize}: The size of the hidden layer in the
      neural network.
    \item
      \texttt{maxEpochs}: The maximum number of epochs (training
      iterations).
    \item
      \texttt{trainingFunctionIdx}: The index of the training function
      to use.
    \item
      \texttt{activationFunctionIdx}: The index of the activation
      function to use.
    \end{itemize}
  \item
    \texttt{x}: The input data (features) for the neural network.
  \item
    \texttt{t}: The target data (labels) for the neural network.
  \item
    \texttt{randomSeed}: The random seed for reproducibility.
  \end{itemize}
\item
  \textbf{Set Random Seed:}

  \begin{itemize}
  \tightlist
  \item
    The function starts by setting the random seed using
    \texttt{rng(randomSeed)} for reproducibility. This ensures that the
    neural network training process is consistent and repeatable.
  \end{itemize}
\item
  \textbf{Extract and Apply Hyperparameters:}

  \begin{itemize}
  \tightlist
  \item
    The function extracts hyperparameters from the input vector
    \texttt{hyperParams}.
  \item
    It uses these hyperparameters to define the neural network
    architecture, training function, and activation function.
  \end{itemize}
\item
  \textbf{Data Splitting:}

  \begin{itemize}
  \tightlist
  \item
    The function defines how to split the data into training,
    validation, and testing sets (70\% for training, 15\% for
    validation, and 15\% for testing).
  \end{itemize}
\item
  \textbf{Train the Neural Network:}

  \begin{itemize}
  \tightlist
  \item
    The function trains the neural network using the specified
    hyperparameters and input data.
  \end{itemize}
\item
  \textbf{Evaluate Performance:}

  \begin{itemize}
  \tightlist
  \item
    The function evaluates the trained network's performance using mean
    squared error (MSE), a measure of prediction accuracy.
  \end{itemize}
\item
  \textbf{Track the Best Trained Network:}

  \begin{itemize}
  \tightlist
  \item
    The function compares the current MSE to the best MSE seen so far.
    If the current MSE is better, the trained network is stored as the
    best-trained network.
  \end{itemize}
\end{enumerate}

\subsubsection{Importance of Random
Seed:}\label{importance-of-random-seed-1}

\begin{itemize}
\tightlist
\item
  \textbf{Reproducibility:} Setting a random seed ensures that the
  neural network training process is reproducible. This allows for
  consistent results across different runs and helps in comparing models
  fairly.
\item
  \textbf{Comparison:} When testing different hyperparameters or models,
  using the same random seed allows for a direct comparison of their
  performance.
\end{itemize}

\subsubsection{Concepts Underlying
Epochs:}\label{concepts-underlying-epochs}

\begin{itemize}
\tightlist
\item
  \textbf{Epochs:} An epoch is a complete pass through the entire
  training dataset. During each epoch, the neural network updates its
  weights based on the training data.
\item
  \textbf{Why Search for Optimal Epochs:} The number of epochs affects
  how much the network learns from the data:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Too Few Epochs:} The network may not learn enough and can
    underfit, performing poorly on new data.
  \item
    \textbf{Too Many Epochs:} The network may learn too much and can
    overfit, performing well on the training data but poorly on new
    data.
  \end{itemize}
\item
  \textbf{Optimum Number of Epochs:} An optimal number of epochs strikes
  a balance between underfitting and overfitting, ensuring the network
  generalizes well to new data.
\end{itemize}

\section{test citation}\label{test-citation}

as dr farid said Khalil et al. (2013) this is no enough

\subsection*{References}\label{references}
\addcontentsline{toc}{subsection}{References}

\vspace{1em}

\textsubscript{Source:
\href{https://MohammedTwheed.github.io/trimming-draft-01/index.qmd.html}{Article
Notebook}}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-khalil-ApplicationArtificialNeural-2013}
Khalil, M., Elgohary, M., \& Shaito, A. (2013). Application of
artificial neural network for the prediction of trimmed impeller size of
a centrifugal pump. In.

\end{CSLReferences}



\end{document}
